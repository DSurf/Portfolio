# Задача
Определение токсичности комментариев.

**Цель:** 
Найти инструмент, который будет искать токсичные комментарии и отправлять их на модерацию.

# Выводы
Данные были загружены и обработаны, текс был очищен, оставлены только буквенные значения. Лемматизация производилась с помощью лематизатора из SpaCy. При подсчете TF-IDF для корпуса текстов исключили стоп-слова.   
Созданы тренеровочная и тестовая и валидационная выборки.   
На них были обучены: LogisticRegression и LGBMClassifier.   
Метрика качества F1 для LogisticRegression на валидационной выборке составила 0.7656707674282366 
Метрика качества F1 для LightGBM на валидационной выборке составила 0.7778561354019746

Метрика качества F1 для LightGBM на тестовой выборке составила 0.7782805429864253
 
Были отобранны случайным порядком 1500 семплов из датасета, для тестирования предобученной модели BERT. 
Метрика качества F1 для пердобученной BERT модели JungleLee составила 0.9747095010252904

# Стек технологий
BERT, Pandas, Python, nltk, tf-idf

# Статус проекта
Завершен
